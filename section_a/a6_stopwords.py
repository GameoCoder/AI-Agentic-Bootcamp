"""
a6_stopwords.py
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Generates a curated Hindi stop word list by:
  1. Asking the Ollama LLM to suggest common Hindi function words.
  2. Cross-referencing with the unigram frequency list (top-N words).
  3. Writing the final merged set to data/stopwords.txt.

Usage:
    python section_a/a6_stopwords.py
"""

import json
import re
import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))
import config
from utils.ollama_client import ollama_chat

# Built-in seed list (always included)
SEED_STOPWORDS = [
    "à¤•à¤¾", "à¤•à¥€", "à¤•à¥‡", "à¤®à¥‡à¤‚", "à¤¹à¥ˆ", "à¤¹à¥ˆà¤‚", "à¤•à¥‹", "à¤¸à¥‡", "à¤ªà¤°", "à¤”à¤°",
    "à¤¯à¤¹", "à¤µà¤¹", "à¤¯à¥‡", "à¤µà¥‡", "à¤œà¥‹", "à¤•à¤¿", "à¤­à¥€", "à¤¨à¥‡", "à¤¤à¥‹", "à¤¹à¥‹",
    "à¤à¤•", "à¤¹à¥€", "à¤¥à¤¾", "à¤¥à¥‡", "à¤¥à¥€", "à¤¯à¤¾", "à¤²à¥‡à¤•à¤¿à¤¨", "à¤…à¤—à¤°", "à¤œà¤¬", "à¤¤à¤¬",
    "à¤•à¤¬", "à¤•à¤¹à¤¾à¤", "à¤•à¥à¤¯à¥‹à¤‚", "à¤•à¥ˆà¤¸à¥‡", "à¤¹à¤®", "à¤†à¤ª", "à¤®à¥ˆà¤‚", "à¤¤à¥à¤®", "à¤µà¥‹",
    "à¤‡à¤¸", "à¤‰à¤¸", "à¤‡à¤¨", "à¤‰à¤¨", "à¤œà¤¿à¤¸", "à¤œà¤¿à¤¨", "à¤‡à¤¸à¥‡", "à¤‰à¤¸à¥‡", "à¤¨à¤¹à¥€à¤‚",
    "à¤…à¤¬", "à¤¤à¤•", "à¤¬à¤¾à¤¦", "à¤ªà¤¹à¤²à¥‡", "à¤¸à¤¾à¤¥", "à¤²à¤¿à¤", "à¤¬à¤¹à¥à¤¤", "à¤•à¥‹à¤ˆ", "à¤•à¥à¤›",
    "à¤¸à¤¬", "à¤¹à¤°", "à¤¦à¥‹à¤¨à¥‹à¤‚", "à¤¦à¥à¤µà¤¾à¤°à¤¾", "à¤°à¤¹à¤¾", "à¤°à¤¹à¥€", "à¤°à¤¹à¥‡", "à¤œà¤¾", "à¤†",
]


def llm_suggest_stopwords() -> list[str]:
    """Ask LLM for common Hindi stop words."""
    prompt = (
        "à¤¹à¤¿à¤¨à¥à¤¦à¥€ à¤­à¤¾à¤·à¤¾ à¤•à¥‡ 60 à¤¸à¤¬à¤¸à¥‡ à¤¸à¤¾à¤®à¤¾à¤¨à¥à¤¯ stop words (function words à¤œà¥ˆà¤¸à¥‡ "
        "à¤¸à¤°à¥à¤µà¤¨à¤¾à¤®, à¤ªà¤°à¤¸à¤°à¥à¤—, à¤¸à¤‚à¤¯à¥‹à¤œà¤¨, à¤•à¥à¤°à¤¿à¤¯à¤¾-à¤¸à¤¹à¤¾à¤¯à¤¿à¤•à¤¾) à¤•à¥€ comma-separated à¤¸à¥‚à¤šà¥€ à¤¦à¥‡à¤‚à¥¤ "
        "à¤•à¥‡à¤µà¤² à¤¶à¤¬à¥à¤¦, à¤•à¥‹à¤ˆ à¤µà¤¿à¤µà¤°à¤£ à¤¨à¤¹à¥€à¤‚à¥¤"
    )
    response = ollama_chat(prompt)
    return [w.strip() for w in re.split(r"[,\n]", response) if w.strip()]


def frequency_based_stopwords(top_n: int = 100) -> list[str]:
    """Return top-N words from unigram list as stop word candidates."""
    if not config.UNIGRAMS_JSON.exists():
        return []
    data = json.loads(config.UNIGRAMS_JSON.read_text(encoding="utf-8"))
    return list(data.keys())[:top_n]


def main():
    print("ðŸŒ± Loading seed stop wordsâ€¦")
    stop_set: set[str] = set(SEED_STOPWORDS)

    print("ðŸ¤– Asking LLM for stop word suggestionsâ€¦")
    llm_words = llm_suggest_stopwords()
    stop_set.update(llm_words)
    print(f"   LLM contributed {len(llm_words)} words")

    print("ðŸ“Š Cross-referencing with unigram frequency listâ€¦")
    freq_candidates = frequency_based_stopwords(top_n=100)
    if freq_candidates:
        # Keep only short words (â‰¤4 chars â†’ likely function words)
        short_freq = [w for w in freq_candidates if len(w) <= 4]
        stop_set.update(short_freq)
        print(f"   Added {len(short_freq)} short high-frequency words")
    else:
        print("   (Unigram list not available yet â€“ skipped)")

    final = sorted(stop_set)
    config.STOPWORDS_TXT.parent.mkdir(parents=True, exist_ok=True)
    config.STOPWORDS_TXT.write_text("\n".join(final), encoding="utf-8")
    print(f"âœ… Stop words saved â†’ {config.STOPWORDS_TXT}  ({len(final)} words)")


if __name__ == "__main__":
    main()
