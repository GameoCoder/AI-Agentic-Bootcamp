"""
c7_word_cloud.py
тФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФА
Hindi Word Cloud Generator (Streamlit app).
  - Upload text or paste content
  - Removes stop words from data/stopwords.txt
  - Generates word cloud image
  - LLM suggests context-specific stop words to remove

Run:
    streamlit run section_c/c7_word_cloud.py
"""

import sys
import json
import io
import re
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))

import streamlit as st
import config
from utils.ollama_client import ollama_chat

st.set_page_config(page_title="рд╣рд┐рдиреНрджреА Word Cloud", page_icon="тШБя╕П", layout="wide")

HINDI_RE = re.compile(r"[\u0900-\u097F]")


@st.cache_data
def load_stopwords() -> set[str]:
    if config.STOPWORDS_TXT.exists():
        return set(config.STOPWORDS_TXT.read_text(encoding="utf-8").splitlines())
    # Minimal built-in
    return {"рдХрд╛", "рдХреА", "рдХреЗ", "рдореЗрдВ", "рд╣реИ", "рд╣реИрдВ", "рдХреЛ", "рд╕реЗ", "рдкрд░", "рдФрд░", "рдпрд╣", "рд╡рд╣", "рдПрдХ"}


def tokenize(text: str) -> list[str]:
    try:
        from indicnlp.tokenize import indic_tokenize
        return indic_tokenize.trivial_tokenize(text, lang="hi")
    except ImportError:
        return text.split()


def compute_freq(text: str, stopwords: set[str], extra_stops: set[str]) -> dict[str, int]:
    from collections import Counter
    all_stops = stopwords | extra_stops
    tokens = tokenize(text)
    filtered = [
        t for t in tokens
        if t not in all_stops and len(t) > 1 and any(HINDI_RE.match(c) for c in t)
    ]
    return dict(Counter(filtered).most_common(200))


def generate_wordcloud(freq: dict[str, int]) -> bytes:
    from wordcloud import WordCloud
    import matplotlib.pyplot as plt

    wc = WordCloud(
        font_path=None,        # system default; user may need to specify a Devanagari font
        width=900, height=500,
        background_color="white",
        colormap="viridis",
        max_words=150,
        prefer_horizontal=0.8,
    ).generate_from_frequencies(freq)

    fig, ax = plt.subplots(figsize=(12, 6))
    ax.imshow(wc, interpolation="bilinear")
    ax.axis("off")
    buf = io.BytesIO()
    fig.savefig(buf, format="png", bbox_inches="tight")
    plt.close(fig)
    return buf.getvalue()


def llm_suggest_extra_stops(text: str, top_words: list[str]) -> list[str]:
    words_str = ", ".join(top_words[:30])
    prompt = (
        f"рдЗрд╕ рд╣рд┐рдиреНрджреА рдкрд╛рда рдХреЗ рд╕рдВрджрд░реНрдн рдореЗрдВ рдЗрди рд╢рдмреНрджреЛрдВ рдореЗрдВ рд╕реЗ рдХреМрди рд╕реЗ word cloud рдХреЗ рд▓рд┐рдП "
        f"рдЕрдиреБрдкрдпреЛрдЧреА/рд╕рд╛рдорд╛рдиреНрдп рд╢рдмреНрдж рд╣реИрдВ?\n\nрд╢рдмреНрдж: {words_str}\n\n"
        "рдХреЗрд╡рд▓ рд╣рдЯрд╛рдиреЗ рдпреЛрдЧреНрдп рд╢рдмреНрджреЛрдВ рдХреА comma-separated рд╕реВрдЪреА рджреЗрдВ (рдЕрдзрд┐рдХрддрдо 10)ред"
    )
    response = ollama_chat(prompt)
    return [w.strip() for w in response.split(",") if w.strip()]


# тФАтФА UI тФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФАтФА
st.title("тШБя╕П рд╣рд┐рдиреНрджреА Word Cloud Generator")

stopwords = load_stopwords()

col1, col2 = st.columns([1, 1])

with col1:
    st.markdown("### рдкрд╛рда рдбрд╛рд▓реЗрдВ")
    uploaded = st.file_uploader("рдлрд╝рд╛рдЗрд▓ рдЕрдкрд▓реЛрдб рдХрд░реЗрдВ (.txt)", type=["txt"])
    if uploaded:
        text_input = uploaded.read().decode("utf-8")
        st.text_area("рдкрд╛рда (рдкреВрд░реНрд╡рд╛рд╡рд▓реЛрдХрди):", text_input[:400] + "тАж", height=120)
    else:
        text_input = st.text_area(
            "рдпрд╣рд╛рдБ рд╣рд┐рдиреНрджреА рдкрд╛рда рдкреЗрд╕реНрдЯ рдХрд░реЗрдВ:",
            value="рднрд╛рд░рдд рдПрдХ рд╡рд┐рд╡рд┐рдзрддрд╛рдУрдВ рд╡рд╛рд▓рд╛ рджреЗрд╢ рд╣реИред рдпрд╣рд╛рдБ рдЕрдиреЗрдХ рднрд╛рд╖рд╛рдПрдБ, рд╕рдВрд╕реНрдХреГрддрд┐рдпрд╛рдБ рдФрд░ рдзрд░реНрдо рд╣реИрдВред",
            height=180,
        )

    extra_stop_input = st.text_input("рдЕрддрд┐рд░рд┐рдХреНрдд stop words (comma-separated):", "")
    extra_stops = {w.strip() for w in extra_stop_input.split(",") if w.strip()}

    use_llm = st.checkbox("ЁЯдЦ LLM рд╕реЗ stop words рд╕реБрдЭрд╛рд╡ рд▓реЗрдВ", value=False)
    generate_btn = st.button("тШБя╕П Word Cloud рдмрдирд╛рдПрдВ", type="primary")

with col2:
    if generate_btn and text_input.strip():
        with st.spinner("Word cloud рдмрдирд╛ рд░рд╣рд╛ рд╣реИтАж"):
            freq = compute_freq(text_input, stopwords, extra_stops)
            if not freq:
                st.warning("рдХреЛрдИ рд╢рдмреНрдж рдирд╣реАрдВ рдорд┐рд▓реЗред stop words рдХрдо рдХрд░реЗрдВред")
            else:
                if use_llm:
                    top_words = list(freq.keys())[:40]
                    llm_stops = llm_suggest_extra_stops(text_input, top_words)
                    st.info(f"LLM рдиреЗ рд╕реБрдЭрд╛рдП stop words: {', '.join(llm_stops)}")
                    freq = {k: v for k, v in freq.items() if k not in llm_stops}

                img_bytes = generate_wordcloud(freq)
                st.image(img_bytes, caption="рд╣рд┐рдиреНрджреА Word Cloud", use_container_width=True)

                # Frequency table
                st.markdown("**рд╢реАрд░реНрд╖ рд╢рдмреНрдж:**")
                st.dataframe(
                    {"рд╢рдмреНрдж": list(freq.keys())[:20], "рдЖрд╡реГрддреНрддрд┐": list(freq.values())[:20]},
                    hide_index=True,
                )
